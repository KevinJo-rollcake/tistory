{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 03 - Multi-variable Linear Regression\n",
    "\n",
    "<img width=\"200\" src=\"https://i.imgur.com/hbPVe1T.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hypothesis and Cost\n",
    "\n",
    "$$ H(x) = Wx + b $$ \n",
    "\n",
    "$$ cost(W, b)=\\frac { 1 }{ m } \\sum _{i=1}^{m}{ { (H{ x }^{ i }-y^{ i } })^{ 2 } }  $$\n",
    "\n",
    "# Simplifed hypothesis\n",
    "\n",
    "$$ H(x) = Wx $$ \n",
    "\n",
    "$$ cost(W)=\\frac { 1 }{ m } \\sum _{i=1}^{m}{ { (W{ x }^{ i }-y^{ i } })^{ 2 } }  $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "b를 W 행렬에 넣어 표현할 수 있기 때문에 생략 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cost function\n",
    "$$ cost(W)=\\frac { 1 }{ m } \\sum _{i=1}^{m}{ { (W{ x }^{ i }-y^{ i } })^{ 2 } }  $$\n",
    "\n",
    "W = -1, cost(W) = 18.67\n",
    "$$ cost(W)=\\frac { 1 }{ 3 } ( (-1 * 1 - 1)^2 + (-1 * 2 - 2)^2 + (-1 * 3 - 3)^2) $$\n",
    "\n",
    "W = 0, cost(W) = 4.67\n",
    "$$ cost(W)=\\frac { 1 }{ 3 } ( (0 * 1 - 1)^2 + (0 * 2 - 2)^2 + (0 * 3 - 3)^2) $$\n",
    "\n",
    "W = 1, cost(W) = 0\n",
    "$$ cost(W)=\\frac { 1 }{ 3 } ( (1 * 1 - 1)^2 + (1 * 2 - 2)^2 + (1 * 3 - 3)^2) $$\n",
    "\n",
    "W = 2, cost(W) = 4.67\n",
    "$$ cost(W)=\\frac { 1 }{ 3 } ( (2 * 1 - 1)^2 + (2 * 2 - 2)^2 + (2 * 3 - 3)^2) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cost function in pure Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.000 |   74.66667\n",
      "-2.429 |   54.85714\n",
      "-1.857 |   38.09524\n",
      "-1.286 |   24.38095\n",
      "-0.714 |   13.71429\n",
      "-0.143 |    6.09524\n",
      " 0.429 |    1.52381\n",
      " 1.000 |    0.00000\n",
      " 1.571 |    1.52381\n",
      " 2.143 |    6.09524\n",
      " 2.714 |   13.71429\n",
      " 3.286 |   24.38095\n",
      " 3.857 |   38.09524\n",
      " 4.429 |   54.85714\n",
      " 5.000 |   74.66667\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = [1, 2, 3]\n",
    "Y = [1, 2, 3]\n",
    "\n",
    "def cost_func(W, X, Y):\n",
    "    c = 0\n",
    "    for i in range(len(X)):\n",
    "        c += (W * X[i] - Y[i]) ** 2\n",
    "    return c / len(X)\n",
    "\n",
    "for feed_W in np.linspace(-3, 5, num=15):\n",
    "    curr_cost = cost_func(feed_W, X, Y)\n",
    "    print(\"%6.3f | %10.5f\" % (feed_W, curr_cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cost function in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.000 |   74.00000\n",
      "-2.429 |   42.00000\n",
      "-1.857 |   18.00000\n",
      "-1.286 |   18.00000\n",
      "-0.714 |    4.00000\n",
      "-0.143 |    4.00000\n",
      " 0.429 |    4.00000\n",
      " 1.000 |    0.00000\n",
      " 1.571 |    0.00000\n",
      " 2.143 |    4.00000\n",
      " 2.714 |    4.00000\n",
      " 3.286 |   18.00000\n",
      " 3.857 |   18.00000\n",
      " 4.429 |   42.00000\n",
      " 5.000 |   74.00000\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "X = [1, 2, 3]\n",
    "Y = [1, 2, 3]\n",
    "\n",
    "W = tf.Variable(0)\n",
    "\n",
    "hypothesis = X * W\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initializers.global_variables())\n",
    "\n",
    "for feed_W in np.linspace(-3, 5, num=15):\n",
    "    curr_cost = sess.run([cost, W], feed_dict={W: feed_W})\n",
    "    print(\"%6.3f | %10.5f\" % (feed_W, curr_cost[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = [1, 2, 3]\n",
    "Y = [1, 2, 3]\n",
    "\n",
    "W = tf.placeholder(tf.float32)\n",
    "\n",
    "hypothesis = X * W\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initializers.global_variables())\n",
    "\n",
    "W_val = np.linspace(-3, 5, num=30)\n",
    "cost_val = []\n",
    "for feed_W in W_val:\n",
    "    curr_cost  = sess.run([cost], feed_dict={W: feed_W})\n",
    "    cost_val.append(curr_cost)\n",
    "\n",
    "plt.plot(W_val, cost_val, \"ro\")\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('W')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How to minimize cost?\n",
    "* 현재 데이터 X와 Y에 대해 W가 1일 때 cost 가 가장 작다\n",
    "* cost 가 최소가 되는 W를 어떻게 찾을 수 있을까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradient descent algorithm\n",
    "* Minimize cost function\n",
    "* used many minimization problems\n",
    "* For a given cost (W, b), it will find W, b to minimize cost\n",
    "* It can be applied to more general function: cost (w1, w2, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How does it work?\n",
    "* Start with initial guesses\n",
    " * Start at 0,0 (or any other value)\n",
    " * Keeping changing $W$ and $b$ a little bit to try and reduce $cost(W,b)$\n",
    "* Each time you change the parameters, you select the gradient which reduces $cost(W,b)$ the most possible \n",
    "* Repeat\n",
    "* Do so until you converge to a local minimum\n",
    "* Has an interesting property\n",
    " * Where you start can determine which minimum you end up\n",
    "\n",
    "http://www.holehouse.org/mlclass/01_02_Introduction_regression_analysis_and_gr.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Formal definition\n",
    "$$ cost(W)=\\frac { 1 }{ m } \\sum _{i=1}^{m}{ { (W{ x }^{ i }-y^{ i } })^{ 2 } }  $$\n",
    "\n",
    "$$ \\Downarrow $$\n",
    "\n",
    "$$ cost(W)=\\frac { 1 }{ 2m } \\sum _{i=1}^{m}{ { (W{ x }^{ i }-y^{ i } })^{ 2 } }  $$\n",
    "\n",
    "* m 혹은 2m 나누는 것이 cost 최소화에 영향 없음\n",
    "* 제곱을 미분할 때, 2가 앞으로 나오면서 공식이 단순하게 되는 효과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Formal definition\n",
    "$$ cost(W)=\\frac { 1 }{ 2m } \\sum _{i=1}^{m}{ { (W{ x }^{ i }-y^{ i } })^{ 2 } }  $$\n",
    "\n",
    "$$ W:=W - \\alpha\\frac{ \\partial } {\\partial W } cost(W) $$\n",
    "\n",
    "* W = W - 변화량\n",
    "* 변화량 = 현 위치(W)에서 비용곡선의 기울기(=미분값) X $\\alpha$ <br> $\\alpha$ : learning rate (시도 간격)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Formal definition\n",
    "\n",
    "$$ W:=W - \\alpha\\frac{ \\partial } {\\partial W } \\frac { 1 }{ 2m } \\sum _{i=1}^{m}{ { (W{ x }^{ i }-y^{ i } })^{ 2 } } $$\n",
    "\n",
    "$$ W:=W-\\alpha \\frac { 1 }{ 2m } \\sum _{ i=1 }^{ m }{ { 2(W{ x }^{ i }-y^{ i } })x^{ i } }  $$\n",
    "\n",
    "$$ W:=W-\\alpha \\frac { 1 }{ m } \\sum _{ i=1 }^{ m }{ { (W{ x }^{ i }-y^{ i } })x^{ i } }  $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradient descent algorithm\n",
    "$$ W:=W-\\alpha \\frac { 1 }{ m } \\sum _{ i=1 }^{ m }{ { (W{ x }^{ i }-y^{ i } })x^{ i } }  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convex function\n",
    "<img width=\"40%\" src=\"http://i.imgur.com/TSKliup.png\" >\n",
    "<img width=\"40%\" src=\"http://i.imgur.com/GyCwshy.png\" >\n",
    "\n",
    "Gradient descent algorithm을 사용하려면, 비용함수 cost(W,b)가 Convex function 이어야 한다\n",
    "\n",
    "http://www.holehouse.org/mlclass/01_02_Introduction_regression_analysis_and_gr.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradient descent 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 | 2820.839355468750000 | 1060951.500000000000000 | 377.778594970703125 | 377.778594970703125\n",
      "  100 | 1.160249233245850 | 0.346157103776932 | 1.821366548538208 | 1.821366548538208\n",
      "  200 | 0.000478029251099 | 0.166666701436043 | 1.666730403900146 | 1.666730403900146\n",
      "  300 | 0.000005960464478 | 0.166666656732559 | 1.666667461395264 | 1.666667461395264\n",
      "  400 | 0.000005960464478 | 0.166666656732559 | 1.666667461395264 | 1.666667461395264\n",
      "  500 | 0.000005960464478 | 0.166666656732559 | 1.666667461395264 | 1.666667461395264\n",
      "  600 | 0.000005960464478 | 0.166666656732559 | 1.666667461395264 | 1.666667461395264\n",
      "  700 | 0.000005960464478 | 0.166666656732559 | 1.666667461395264 | 1.666667461395264\n",
      "  800 | 0.000005960464478 | 0.166666656732559 | 1.666667461395264 | 1.666667461395264\n",
      "  900 | 0.000005960464478 | 0.166666656732559 | 1.666667461395264 | 1.666667461395264\n",
      " 1000 | 0.000005960464478 | 0.166666656732559 | 1.666667461395264 | 1.666667461395264\n",
      " 1100 | 0.000005960464478 | 0.166666656732559 | 1.666667461395264 | 1.666667461395264\n",
      " 1200 | 0.000005960464478 | 0.166666656732559 | 1.666667461395264 | 1.666667461395264\n",
      " 1300 | 0.000005960464478 | 0.166666656732559 | 1.666667461395264 | 1.666667461395264\n",
      " 1400 | 0.000005960464478 | 0.166666656732559 | 1.666667461395264 | 1.666667461395264\n",
      " 1500 | 0.000005960464478 | 0.166666656732559 | 1.666667461395264 | 1.666667461395264\n",
      " 1600 | 0.000005960464478 | 0.166666656732559 | 1.666667461395264 | 1.666667461395264\n",
      " 1700 | 0.000005960464478 | 0.166666656732559 | 1.666667461395264 | 1.666667461395264\n",
      " 1800 | 0.000005960464478 | 0.166666656732559 | 1.666667461395264 | 1.666667461395264\n",
      " 1900 | 0.000005960464478 | 0.166666656732559 | 1.666667461395264 | 1.666667461395264\n",
      "--------------------------------------------------\n",
      "[8.333338]\n",
      "[4.166669]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x_data = [1., 2., 3., 4.]\n",
    "y_data = [1., 3., 5., 7.]\n",
    "\n",
    "W = tf.Variable(tf.random_uniform([1], -1000., 1000.))\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "hypothesis = W * X\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "mean = tf.reduce_mean(tf.multiply(tf.multiply(W, X) - Y, X))\n",
    "descent = W - tf.multiply(0.01, mean)\n",
    "\n",
    "# W udpate\n",
    "update  = W.assign(descent) \n",
    "\n",
    "init = tf.initializers.global_variables()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(2000):\n",
    "    uResult = sess.run(update, feed_dict={X: x_data, Y: y_data}) # update W\n",
    "    cResult = sess.run(cost, feed_dict={X: x_data, Y: y_data})\n",
    "    wResult = sess.run(W)\n",
    "    mResult = sess.run(mean, feed_dict={X: x_data, Y: y_data})\n",
    "    if step % 100 == 0:\n",
    "        print('%5d | %.15f | %.15f | %.15f | %.15f' %(step, mResult, cResult, wResult, uResult))\n",
    "        \n",
    "print('-' * 50)\n",
    "print(sess.run(hypothesis, feed_dict={X: 5.0}))\n",
    "print(sess.run(hypothesis, feed_dict={X: 2.5}))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Liner regression Summary\n",
    "\n",
    "## 1) Hypothesis \n",
    "\n",
    "$$ H(x) = Wx + b $$\n",
    "\n",
    "## 2) Cost function\n",
    "\n",
    "$$ cost(W)=\\frac { 1 }{ m } \\sum _{i=1}^{m}{ { (W{ x }^{ i }-y^{ i } })^{ 2 } }  $$\n",
    "\n",
    "## 3) Gradient descent\n",
    "\n",
    "$$ W := W-\\alpha \\frac { \\partial  }{ \\partial W } cost(W) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Multi-variable linear regression\n",
    "Predicting exam score - regression using three inputs (x1, x2, x3)\n",
    "\n",
    "x1 (quiz 1) | x2 (quiz 2) | x3 (mid 1) | Y (final)\n",
    "---- | ---- | ----| ----\n",
    "73 | 80 | 75 | 152\n",
    "93 | 88 | 93 | 185\n",
    "89 | 91 | 90 | 180\n",
    "96 | 98 | 100 | 196\n",
    "73 | 66 | 70 | 142\n",
    "\n",
    "\n",
    "Test Scores for General Psychology ( https://goo.gl/g2T8Kp )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Matrix multiplication\n",
    "\n",
    "## dot product(=scalar product, 내적)\n",
    "<img src=\"https://www.mathsisfun.com/algebra/images/matrix-multiply-a.svg\" >\n",
    "\n",
    "\n",
    "https://www.mathsisfun.com/algebra/matrix-multiplying.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multi-feature regression\n",
    "\n",
    "### Hypothesis\n",
    "\n",
    "$$ H(x) = w x + b $$\n",
    "\n",
    "$$ H(x_1, x_2, x_3) = w_1 x_1 + w_2 x_2 + w_3 x_3 + b $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hypothesis using matrix\n",
    "\n",
    "$$ H(x_1, x_2, x_3) = \\underline{w_1 x_1 + w_2 x_2 + w_3 x_3} + b $$\n",
    "\n",
    "$$ w_1 x_1 + w_2 x_2 + w_3 x_3 $$ \n",
    "\n",
    "$$ \\begin{pmatrix} w_{ 1 } & w_{ 2 } & w_{ 3 } \\end{pmatrix}\\cdot \\begin{pmatrix} x_{ 1 } \\\\ x_{ 2 } \\\\ x_{ 3 } \\end{pmatrix} $$\n",
    "\n",
    "$$ WX $$ (W, X 는 matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hypothesis without b\n",
    "\n",
    "$$ H(x_1, x_2, x_3) = w_1 x_1 + w_2 x_2 + w_3 x_3 + b$$\n",
    "\n",
    "$$ = b + w_1 x_1 + w_2 x_2 + w_3 x_3 $$\n",
    "\n",
    "$$ = \\begin{pmatrix} b & x_{ 1 } & x_{ 2 } & x_{ 3 } \\end{pmatrix}\\cdot \\begin{pmatrix} 1 \\\\ w_{ 1 } \\\\ w_{ 2 } \\\\ w_{ 3 } \\end{pmatrix} $$\n",
    "\n",
    "$$ = XW $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hypothesis using matrix \n",
    "\n",
    "### Many x instances\n",
    "\n",
    "$$ \\begin{pmatrix} x_{ 11 } & x_{ 12 } & x_{ 13 } \\\\ x_{ 21 } & x_{ 22 } & x_{ 23 } \\\\ x_{ 31 } & x_{ 32 } & x_{ 33 }\\\\ x_{ 41 } & x_{ 42 } & x_{ 43 }\\\\ x_{ 51 } & x_{ 52 } & x_{ 53 }\\end{pmatrix} \\cdot \\begin{pmatrix} w_{ 1 } \\\\ w_{ 2 } \\\\ w_{ 3 } \\end{pmatrix}=\\begin{pmatrix} x_{ 11 }w_{ 1 }+x_{ 12 }w_{ 2 }+x_{ 13 }w_{ 3 } \\\\ x_{ 21 }w_{ 1 }+x_{ 22 }w_{ 2 }+x_{ 23 }w_{ 3 }\\\\ x_{ 31 }w_{ 1 }+x_{ 32 }w_{ 2 }+x_{ 33 }w_{ 3 } \\\\ x_{ 41 }w_{ 1 }+x_{ 42 }w_{ 2 }+x_{ 43 }w_{ 3 } \\\\ x_{ 51 }w_{ 1 }+x_{ 52 }w_{ 2 }+x_{ 53 }w_{ 3 } \\end{pmatrix} $$\n",
    "\n",
    "$$ [5, 3] \\cdot [3, 1] = [5, 1] $$\n",
    "\n",
    "$$ H(X) = XW $$\n",
    "\n",
    "5는 데이터(instance)의 수, 3은 변수(feature)의 수, 1은 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hypothesis using matrix (n output)\n",
    "\n",
    "$$ [n, 3] \\cdot [?, ?] = [n, 2] $$\n",
    "\n",
    "$$ H(X) = XW $$\n",
    "\n",
    "* n은 데이터(instance)의 개수, 2는 결과 값의 개수로 주어진다.\n",
    "* 이때, W [?, ?] ⇒ [3, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# WX vs XW\n",
    "\n",
    "### Theory (Lecture) :\n",
    " $$ H(x) = Wx + b  $$\n",
    "\n",
    "### TensorFlow (Implementation) :\n",
    "\n",
    "$$ H(X) = XW $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Simple Example (2 variables)\n",
    "\n",
    "x1 | x2 | y\n",
    "---- | ---- | ----\n",
    "1  |  0  |  1\n",
    "0  |  2  |  2\n",
    "3  |  0  |  3\n",
    "0  |  4  |  4\n",
    "5  |  0  |  5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100   | 0.000457131478470 | 0.986676 | 0.984191 | 0.050686\n",
      "200   | 0.000000930154215 | 0.999399 | 0.999287 | 0.002286\n",
      "300   | 0.000000001895285 | 0.999973 | 0.999968 | 0.000103\n",
      "400   | 0.000000000003899 | 0.999999 | 0.999999 | 0.000005\n",
      "500   | 0.000000000000026 | 1.000000 | 1.000000 | 0.000000\n",
      "600   | 0.000000000000048 | 1.000000 | 1.000000 | 0.000000\n",
      "700   | 0.000000000000048 | 1.000000 | 1.000000 | 0.000000\n",
      "800   | 0.000000000000048 | 1.000000 | 1.000000 | 0.000000\n",
      "900   | 0.000000000000048 | 1.000000 | 1.000000 | 0.000000\n",
      "1000  | 0.000000000000048 | 1.000000 | 1.000000 | 0.000000\n",
      "1100  | 0.000000000000048 | 1.000000 | 1.000000 | 0.000000\n",
      "1200  | 0.000000000000048 | 1.000000 | 1.000000 | 0.000000\n",
      "1300  | 0.000000000000048 | 1.000000 | 1.000000 | 0.000000\n",
      "1400  | 0.000000000000048 | 1.000000 | 1.000000 | 0.000000\n",
      "1500  | 0.000000000000048 | 1.000000 | 1.000000 | 0.000000\n",
      "1600  | 0.000000000000048 | 1.000000 | 1.000000 | 0.000000\n",
      "1700  | 0.000000000000048 | 1.000000 | 1.000000 | 0.000000\n",
      "1800  | 0.000000000000048 | 1.000000 | 1.000000 | 0.000000\n",
      "1900  | 0.000000000000048 | 1.000000 | 1.000000 | 0.000000\n",
      "2000  | 0.000000000000048 | 1.000000 | 1.000000 | 0.000000\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x1_data = [1, 0, 3, 0, 5]\n",
    "x2_data = [0, 2, 0, 4, 0]\n",
    "y_data  = [1, 2, 3, 4, 5]\n",
    "\n",
    "W1 = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "W2 = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "b  = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "\n",
    "# hypothesis = W * X + b\n",
    "hypothesis = W1 * x1_data + W2 * x2_data + b\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "\n",
    "rate = tf.Variable(0.1)\n",
    "optimizer = tf.train.GradientDescentOptimizer(rate)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "init = tf.initializers.global_variables()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(1,2001):\n",
    "    sess.run(train)\n",
    "    if step % 100 == 0:\n",
    "        print(\"%-5d | %.15f | %f | %f | %f\" % (step, sess.run(cost), sess.run(W1), sess.run(W2), sess.run(b)))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Simple Example (2 variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100   | 0.000288817245746 | [[0.9894093  0.98743427]] | 0.040288 \n",
      "200   | 0.000000587746285 | [[0.99952227 0.99943316]] | 0.001817 \n",
      "300   | 0.000000001193442 | [[0.9999784 0.9999744]] | 0.000082 \n",
      "400   | 0.000000000002478 | [[0.9999991  0.99999887]] | 0.000004 \n",
      "500   | 0.000000000000080 | [[1.         0.99999994]] | 0.000000 \n",
      "600   | 0.000000000000014 | [[1.         0.99999994]] | 0.000000 \n",
      "700   | 0.000000000000014 | [[1.         0.99999994]] | 0.000000 \n",
      "800   | 0.000000000000014 | [[1.         0.99999994]] | 0.000000 \n",
      "900   | 0.000000000000014 | [[1.         0.99999994]] | 0.000000 \n",
      "1000  | 0.000000000000014 | [[1.         0.99999994]] | 0.000000 \n",
      "1100  | 0.000000000000014 | [[1.         0.99999994]] | 0.000000 \n",
      "1200  | 0.000000000000014 | [[1.         0.99999994]] | 0.000000 \n",
      "1300  | 0.000000000000014 | [[1.         0.99999994]] | 0.000000 \n",
      "1400  | 0.000000000000014 | [[1.         0.99999994]] | 0.000000 \n",
      "1500  | 0.000000000000014 | [[1.         0.99999994]] | 0.000000 \n",
      "1600  | 0.000000000000014 | [[1.         0.99999994]] | 0.000000 \n",
      "1700  | 0.000000000000014 | [[1.         0.99999994]] | 0.000000 \n",
      "1800  | 0.000000000000014 | [[1.         0.99999994]] | 0.000000 \n",
      "1900  | 0.000000000000014 | [[1.         0.99999994]] | 0.000000 \n",
      "2000  | 0.000000000000014 | [[1.         0.99999994]] | 0.000000 \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x_data = [\n",
    "    [1., 0., 3., 0., 5.],\n",
    "    [0., 2., 0., 4., 0.]\n",
    "]\n",
    "y_data  = [1, 2, 3, 4, 5]\n",
    "\n",
    "W = tf.Variable(tf.random_uniform([1, 2], -1.0, 1.0))\n",
    "b = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "\n",
    "hypothesis = tf.matmul(W, x_data) + b     # [1, 2] * [2, 5] = [1, 5]\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "\n",
    "rate = tf.Variable(0.1)\n",
    "optimizer = tf.train.GradientDescentOptimizer(rate)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initializers.global_variables())\n",
    "\n",
    "for step in range(1,2001):\n",
    "    sess.run(train)\n",
    "    if step % 100 == 0:\n",
    "        print(\"%-5d | %.15f | %s | %f \" % (step, sess.run(cost), sess.run(W), sess.run(b)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hypothesis without b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50    | 0.005488837137818 | [[0.1756326 0.9538311 0.945221 ]] | -0.119597 \n",
      "100   | 0.000247604621109 | [[0.03730303 0.9901941  0.98836535]] | -0.119597 \n",
      "150   | 0.000011169533536 | [[0.00792286 0.99791735 0.9975289 ]] | -0.119597 \n",
      "200   | 0.000000503816011 | [[0.00168275 0.9995577  0.9994751 ]] | -0.119597 \n",
      "250   | 0.000000022737765 | [[3.5738791e-04 9.9990606e-01 9.9988854e-01]] | -0.119597 \n",
      "300   | 0.000000001024549 | [[7.5920412e-05 9.9998003e-01 9.9997634e-01]] | -0.119597 \n",
      "350   | 0.000000000045978 | [[1.6077338e-05 9.9999577e-01 9.9999499e-01]] | -0.119597 \n",
      "400   | 0.000000000002004 | [[3.403004e-06 9.999991e-01 9.999989e-01]] | -0.119597 \n",
      "450   | 0.000000000000114 | [[7.5655805e-07 9.9999976e-01 9.9999976e-01]] | -0.119597 \n",
      "500   | 0.000000000000003 | [[1.8435321e-07 9.9999994e-01 9.9999994e-01]] | -0.119597 \n",
      "550   | 0.000000000000000 | [[1.1282769e-07 9.9999994e-01 1.0000000e+00]] | -0.119597 \n",
      "600   | 0.000000000000000 | [[1.1282769e-07 9.9999994e-01 1.0000000e+00]] | -0.119597 \n",
      "650   | 0.000000000000000 | [[1.1282769e-07 9.9999994e-01 1.0000000e+00]] | -0.119597 \n",
      "700   | 0.000000000000000 | [[1.1282769e-07 9.9999994e-01 1.0000000e+00]] | -0.119597 \n",
      "750   | 0.000000000000000 | [[1.1282769e-07 9.9999994e-01 1.0000000e+00]] | -0.119597 \n",
      "800   | 0.000000000000000 | [[1.1282769e-07 9.9999994e-01 1.0000000e+00]] | -0.119597 \n",
      "850   | 0.000000000000000 | [[1.1282769e-07 9.9999994e-01 1.0000000e+00]] | -0.119597 \n",
      "900   | 0.000000000000000 | [[1.1282769e-07 9.9999994e-01 1.0000000e+00]] | -0.119597 \n",
      "950   | 0.000000000000000 | [[1.1282769e-07 9.9999994e-01 1.0000000e+00]] | -0.119597 \n",
      "1000  | 0.000000000000000 | [[1.1282769e-07 9.9999994e-01 1.0000000e+00]] | -0.119597 \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 앞의 코드에서 bias(b)를 행렬에 추가\n",
    "# 갯수가 같아야 하므로 b를 리스트로 처리\n",
    "\n",
    "x_data = [\n",
    "    [1., 1., 1., 1., 1.], \n",
    "    [1., 0., 3., 0., 5.], \n",
    "    [0., 2., 0., 4., 0.]\n",
    "]\n",
    "y_data  = [1, 2, 3, 4, 5]\n",
    "\n",
    "W = tf.Variable(tf.random_uniform([1, 3], -1.0, 1.0)) # [1, 3]으로 변경하고, b 삭제\n",
    "hypothesis = tf.matmul(W, x_data) # b가 없다\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "\n",
    "rate = tf.Variable(0.1)\n",
    "optimizer = tf.train.GradientDescentOptimizer(rate)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "init = tf.initializers.global_variables()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(1,1001):\n",
    "    sess.run(train)\n",
    "    if step % 50 == 0:\n",
    "        # without b\n",
    "        print(\"%-5d | %.15f | %s | %f \" % (step, sess.run(cost), sess.run(W), sess.run(b)))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-variable linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0: Cose=2667.6282 Prediction:  [105.34478  129.50781  125.94277  139.99611   97.017456]\n",
      " 100: Cose=  3.8105 Prediction:  [151.11192 184.57016 180.1694  199.04204 139.03413]\n",
      " 200: Cose=  3.7618 Prediction:  [151.08475 184.58958 180.16211 199.02852 139.06659]\n",
      " 300: Cose=  3.7151 Prediction:  [151.05843 184.60847 180.15512 199.01526 139.09831]\n",
      " 400: Cose=  3.6701 Prediction:  [151.03287 184.62682 180.14833 199.00217 139.12927]\n",
      " 500: Cose=  3.6269 Prediction:  [151.0081  184.64462 180.1418  198.98927 139.15953]\n",
      " 600: Cose=  3.5852 Prediction:  [150.9841  184.66193 180.1355  198.97658 139.1891 ]\n",
      " 700: Cose=  3.5450 Prediction:  [150.96082 184.6787  180.1294  198.96404 139.21799]\n",
      " 800: Cose=  3.5063 Prediction:  [150.93826 184.695   180.12355 198.9517  139.24623]\n",
      " 900: Cose=  3.4689 Prediction:  [150.91638 184.7108  180.11787 198.9395  139.27379]\n",
      "1000: Cose=  3.4329 Prediction:  [150.89517 184.72614 180.1124  198.92747 139.30074]\n",
      "1100: Cose=  3.3981 Prediction:  [150.87462 184.74104 180.10715 198.91562 139.32707]\n",
      "1200: Cose=  3.3644 Prediction:  [150.85472 184.7555  180.10208 198.90393 139.35283]\n",
      "1300: Cose=  3.3319 Prediction:  [150.83545 184.76955 180.0972  198.89241 139.378  ]\n",
      "1400: Cose=  3.3004 Prediction:  [150.81677 184.78314 180.0925  198.88101 139.40262]\n",
      "1500: Cose=  3.2700 Prediction:  [150.79869 184.79634 180.088   198.86978 139.42668]\n",
      "1600: Cose=  3.2405 Prediction:  [150.78116 184.80917 180.08365 198.85869 139.4502 ]\n",
      "1700: Cose=  3.2119 Prediction:  [150.76419 184.8216  180.07947 198.84772 139.4732 ]\n",
      "1800: Cose=  3.1842 Prediction:  [150.74779 184.83365 180.07547 198.83693 139.49571]\n",
      "1900: Cose=  3.1573 Prediction:  [150.73192 184.84537 180.07162 198.82626 139.51773]\n",
      "2000: Cose=  3.1312 Prediction:  [150.71652 184.85667 180.0679  198.81569 139.53925]\n",
      "2100: Cose=  3.1058 Prediction:  [150.70163 184.86768 180.06435 198.80527 139.5603 ]\n",
      "2200: Cose=  3.0811 Prediction:  [150.68726 184.87836 180.06096 198.79497 139.58092]\n",
      "2300: Cose=  3.0572 Prediction:  [150.67331 184.88867 180.05768 198.78479 139.60106]\n",
      "2400: Cose=  3.0338 Prediction:  [150.65987 184.8987  180.05458 198.77477 139.62082]\n",
      "2500: Cose=  3.0111 Prediction:  [150.64684 184.90839 180.05157 198.7648  139.6401 ]\n",
      "2600: Cose=  2.9889 Prediction:  [150.63426 184.91782 180.04872 198.75499 139.65901]\n",
      "2700: Cose=  2.9673 Prediction:  [150.6221  184.92693 180.04599 198.74527 139.6775 ]\n",
      "2800: Cose=  2.9462 Prediction:  [150.61037 184.93578 180.0434  198.73569 139.69563]\n",
      "2900: Cose=  2.9256 Prediction:  [150.59901 184.94432 180.04091 198.72615 139.71335]\n",
      "3000: Cose=  2.9055 Prediction:  [150.58806 184.9526  180.03854 198.71677 139.73073]\n",
      "3100: Cose=  2.8859 Prediction:  [150.57747 184.96065 180.03629 198.70746 139.74773]\n",
      "3200: Cose=  2.8667 Prediction:  [150.56726 184.96841 180.03413 198.69826 139.76437]\n",
      "3300: Cose=  2.8479 Prediction:  [150.5574  184.97594 180.03209 198.68916 139.7807 ]\n",
      "3400: Cose=  2.8295 Prediction:  [150.5479  184.98323 180.03017 198.68015 139.79669]\n",
      "3500: Cose=  2.8115 Prediction:  [150.53874 184.99028 180.02832 198.67123 139.81235]\n",
      "3600: Cose=  2.7938 Prediction:  [150.5299  184.9971  180.0266  198.66241 139.8277 ]\n",
      "3700: Cose=  2.7765 Prediction:  [150.5214  185.00372 180.02498 198.65369 139.84274]\n",
      "3800: Cose=  2.7596 Prediction:  [150.5132  185.0101  180.02342 198.64503 139.8575 ]\n",
      "3900: Cose=  2.7429 Prediction:  [150.50528 185.01628 180.02197 198.63647 139.87195]\n",
      "4000: Cose=  2.7265 Prediction:  [150.49767 185.02223 180.02058 198.62796 139.8861 ]\n"
     ]
    }
   ],
   "source": [
    "# Multi-variable linear regression\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.set_random_seed(0)  # for reproducibility\n",
    "\n",
    "data = np.array([\n",
    "    [73,80,75,152],\n",
    "    [93,88,93,185],\n",
    "    [89,91,90,180],\n",
    "    [96,98,100,196],\n",
    "    [73,66,70,142]\n",
    "])\n",
    "\n",
    "x1_data = data[:,0]\n",
    "x2_data = data[:,1]\n",
    "x3_data = data[:,2]\n",
    "y_data = data[:,3]\n",
    "\n",
    "# placeholders for a tensor that will be always fed\n",
    "x1 = tf.placeholder(tf.float32)\n",
    "x2 = tf.placeholder(tf.float32)\n",
    "x3 = tf.placeholder(tf.float32)\n",
    "\n",
    "Y = tf.placeholder(tf.float32)\n",
    "w1 = tf.Variable(tf.random_normal([1]), name='weight1')\n",
    "w2 = tf.Variable(tf.random_normal([1]), name='weight2')\n",
    "w3 = tf.Variable(tf.random_normal([1]), name='weight3')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "hypothesis = x1 * w1 + x2 * w2 + x3 * w3 + b\n",
    "\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize cost\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initializers.global_variables())\n",
    "\n",
    "for step in range(4001):\n",
    "    cost_val, hy_val, _ = sess.run([cost, hypothesis, train],\n",
    "                          feed_dict={x1: x1_data, x2: x2_data, x3: x3_data, Y: y_data})\n",
    "    if step % 100 == 0:\n",
    "        print('%4d: Cose=%8.4f' % (step, cost_val), \"Prediction: \", hy_val)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
