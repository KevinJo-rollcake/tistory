{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텐서플로우 Eager + Keras\n",
    "\n",
    " - 공식 텐서플로우 예제를 한글 데이터로 변환하였습니다. 원 소스는 아래를 참고하세요\n",
    " \n",
    "# Copyright 2018 The TensorFlow Authors.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\").\n",
    "\n",
    "# Neural Machine Translation with Attention\n",
    "\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\"><td>\n",
    "<a target=\"_blank\"  href=\"https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>  \n",
    "</td><td>\n",
    "<a target=\"_blank\"  href=\"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb\"><img width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a></td></table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "tnxXKDjq3jEL"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "tf.enable_eager_execution must be called at program startup.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-db990ecc5dce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_eager_execution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf110/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36menable_eager_execution\u001b[0;34m(config, device_policy, execution_mode)\u001b[0m\n\u001b[1;32m   5185\u001b[0m   \"\"\"\n\u001b[1;32m   5186\u001b[0m   return enable_eager_execution_internal(\n\u001b[0;32m-> 5187\u001b[0;31m       config, device_policy, execution_mode, None)\n\u001b[0m\u001b[1;32m   5188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf110/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36menable_eager_execution_internal\u001b[0;34m(config, device_policy, execution_mode, server_def)\u001b[0m\n\u001b[1;32m   5253\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5254\u001b[0m     raise ValueError(\n\u001b[0;32m-> 5255\u001b[0;31m         \"tf.enable_eager_execution must be called at program startup.\")\n\u001b[0m\u001b[1;32m   5256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5257\u001b[0m   \u001b[0;31m# Monkey patch to get rid of an unnecessary conditional since the context is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: tf.enable_eager_execution must be called at program startup."
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "# Import TensorFlow >= 1.10 and enable eager execution\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Twitter\n",
    "import pandas as pd\n",
    "import enum\n",
    "import os\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData():\n",
    "    # 판다스를 통해서 데이터를 불러온다.\n",
    "    dataDF = pd.read_csv('./data_in/chat_ko/ChatBotData.csv', header=0)\n",
    "    # 질문과 답변 열을 가져와 question과 answer에 넣는다.\n",
    "    question, answer = list(dataDF['Q']), list(dataDF['A'])\n",
    "#     dataset = dataDF['Q'] + '\\t' + dataDF['A']\n",
    "#     dataset = list(dataset)\n",
    "    # skleran에서 지원하는 함수를 통해서 학습 셋과 \n",
    "    # 테스트 셋을 나눈다.\n",
    "#     xTrain, xTest, yTrain, yTest = train_test_split(question, answer, test_size=0.33, random_state=42)\n",
    "    # 그 값을 리턴한다.\n",
    "#     return xTrain, yTrain, xTest, yTest\n",
    "    return question, answer\n",
    "\n",
    "def preproLikeMorphlized(data):\n",
    "    # 형태소 분석 모듈 객체를\n",
    "    # 생성합니다.\n",
    "\n",
    "    morphAnalyzer = Twitter()\n",
    "    # 형태소 토크나이즈 결과 문장을 받을\n",
    "    #  리스트를 생성합니다.\n",
    "    result_data = list()\n",
    "    # 데이터에 있는 매 문장에 대해 토크나이즈를\n",
    "    # 할 수 있도록 반복문을 선언합니다.\n",
    "    for seq in data:\n",
    "        # Twitter.morphs 함수를 통해 토크나이즈 된\n",
    "        # 리스트 객체를 받고 다시 공백문자를 기준으로\n",
    "        # 하여 문자열로 재구성 해줍니다.\n",
    "        morphlizedSeq = \" \".join(morphAnalyzer.morphs(seq.replace(' ', '')))\n",
    "        result_data.append(morphlizedSeq)\n",
    "\n",
    "    return result_data\n",
    "\n",
    "def dataTokenizer(data):\n",
    "    # 토크나이징 해서 담을 배열 생성\n",
    "    words = []\n",
    "    for sentence in data:\n",
    "        # FILTERS = \"([~.,!?\\\"':;)(])\"\n",
    "        # 위 필터와 같은 값들을 정규화 표현식을 \n",
    "        # 통해서 모두 \"\" 으로 변환 해주는 부분이다.\n",
    "        sentence = re.sub(change_filter, \"\", sentence)\n",
    "        for word in sentence.split():\n",
    "            words.append(word)\n",
    "    # 토그나이징과 정규표현식을 통해 만들어진 \n",
    "    # 값들을 넘겨 준다.\n",
    "    return [word for word in words if word]\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    \n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\" \n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    \n",
    "    w = w.rstrip().strip()\n",
    "    \n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    \n",
    "    return w\n",
    "\n",
    "def create_dataset(data, num_examples):\n",
    "#     word_pairs = [[preprocess_sentence(w) for w in l]  for l in data[:num_examples]]\n",
    "    word_pairs = [[preprocess_sentence(l)]  for l in data[:num_examples]]\n",
    "    return word_pairs\n",
    "\n",
    "class LanguageIndex():\n",
    "    def __init__(self, lang):\n",
    "        self.lang = lang\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.vocab = set()\n",
    "\n",
    "        self.create_index()\n",
    "        \n",
    "    def create_index(self):\n",
    "        for phrase in self.lang:\n",
    "#             if self.vocab == '<start>':\n",
    "#                 pass\n",
    "            self.vocab.update(phrase.split(' '))\n",
    "    \n",
    "        self.vocab = sorted(self.vocab)\n",
    "    \n",
    "        self.word2idx['<pad>'] = 0\n",
    "#         self.word2idx['<start>'] = 1\n",
    "#         self.word2idx['<end>'] = 2\n",
    "#         self.word2idx['<unk>'] = 3\n",
    "\n",
    "        for index, word in enumerate(self.vocab):\n",
    "            self.word2idx[word] = index + 1\n",
    "\n",
    "        for word, index in self.word2idx.items():\n",
    "            self.idx2word[index] = word\n",
    "            \n",
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, num_examples):\n",
    "    \n",
    "    # 판다스를 통해서 데이터를 불러온다.\n",
    "    data_df = pd.read_csv(path, header=0)\n",
    "    # 질문과 답변 열을 가져와 question과 answer에 넣는다.\n",
    "    question, answer = list(data_df['Q']), list(data_df['A'])\n",
    "    \n",
    "    question = create_dataset(question, num_examples)\n",
    "    answer = create_dataset(answer, num_examples)\n",
    "    \n",
    "    print(len(question))\n",
    "    \n",
    "    pairs = [question[i] + answer[i] for i in range(num_examples)]\n",
    "    \n",
    "    # index language using the class defined above    \n",
    "    inp_lang = LanguageIndex(q for q, a in pairs)\n",
    "    targ_lang = LanguageIndex(a for q, a in pairs)\n",
    "    \n",
    "    # Vectorize the input and target languages\n",
    "    \n",
    "    # question\n",
    "    input_tensor = [[inp_lang.word2idx[s] for s in q.split(' ')] for q, a in pairs]\n",
    "    \n",
    "    # answer\n",
    "    target_tensor = [[targ_lang.word2idx[s] for s in a.split(' ')] for q, a in pairs]\n",
    "\n",
    "    # Calculate max_length of input and output tensor\n",
    "    # Here, we'll set those to the longest sentence in the dataset\n",
    "    max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
    "    \n",
    "    # Padding the input and output tensor to the maximum length\n",
    "    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n",
    "                                                                 maxlen=max_length_inp,\n",
    "                                                                 padding='post')\n",
    "    \n",
    "    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n",
    "                                                                  maxlen=max_length_tar, \n",
    "                                                                  padding='post')\n",
    "    \n",
    "    return input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_tar    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'./data_in/ChatBotData.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-66030da81f31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnum_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m11823\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length_targ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data_in/ChatBotData.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Creating training and validation sets using an 80-20 split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-6e835b0c29ea>\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, num_examples)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# 판다스를 통해서 데이터를 불러온다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdata_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m# 질문과 답변 열을 가져와 question과 answer에 넣는다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Q'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'A'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf110/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf110/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf110/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf110/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf110/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'./data_in/ChatBotData.csv' does not exist"
     ]
    }
   ],
   "source": [
    "# Load dataset with limit\n",
    "\n",
    "num_examples = 11823\n",
    "\n",
    "input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_targ = load_dataset('./data_in/ChatBotData.csv', num_examples)\n",
    "\n",
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rgCLkfv5uO3d"
   },
   "source": [
    "### Create a tf.data dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TqHsArVZ3jFS"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word2idx)\n",
    "vocab_tar_size = len(targ_lang.word2idx)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True) #drop_reainder가 True이면, batch size가 적지 않도록 수정\n",
    "\n",
    "# The tensors in the resulting element will have an additional outer dimension, which will be batch_size (or N % batch_size for the last element if batch_size does not divide the number of input elements N evenly and drop_remainder is False). If your program depends on the batches having the same outer dimension, you should set the drop_remainder argument to True to prevent the smaller batch from being produced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TNfHIF71ulLu"
   },
   "source": [
    "## Write the encoder and decoder model\n",
    "\n",
    "Here, we'll implement an encoder-decoder model with attention which you can read about in the TensorFlow [Neural Machine Translation (seq2seq) tutorial](https://www.tensorflow.org/tutorials/seq2seq). This example uses a more recent set of APIs. This notebook implements the [attention equations](https://www.tensorflow.org/tutorials/seq2seq#background_on_the_attention_mechanism) from the seq2seq tutorial. The following diagram shows that each input words is assigned a weight by the attention mechanism which is then used by the decoder to predict the next word in the sentence.\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_mechanism.jpg\" width=\"500\" alt=\"attention mechanism\">\n",
    "\n",
    "The input is put through an encoder model which gives us the encoder output of shape *(batch_size, max_length, hidden_size)* and the encoder hidden state of shape *(batch_size, hidden_size)*. \n",
    "\n",
    "Here are the equations that are implemented:\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_equation_0.jpg\" alt=\"attention equation 0\" width=\"800\">\n",
    "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_equation_1.jpg\" alt=\"attention equation 1\" width=\"800\">\n",
    "\n",
    "We're using *Bahdanau attention*. Lets decide on notation before writing the simplified form:\n",
    "\n",
    "* FC = Fully connected (dense) layer\n",
    "* EO = Encoder output\n",
    "* H = hidden state\n",
    "* X = input to the decoder\n",
    "\n",
    "And the pseudo-code:\n",
    "\n",
    "* `score = FC(tanh(FC(EO) + FC(H)))`\n",
    "* `attention weights = softmax(score, axis = 1)`. Softmax by default is applied on the last axis but here we want to apply it on the *1st axis*, since the shape of score is *(batch_size, max_length, 1)*. `Max_length` is the length of our input. Since we are trying to assign a weight to each input, softmax should be applied on that axis.\n",
    "* `context vector = sum(attention weights * EO, axis = 1)`. Same reason as above for choosing axis as 1.\n",
    "* `embedding output` = The input to the decoder X is passed through an embedding layer.\n",
    "* `merged vector = concat(embedding output, context vector)`\n",
    "* This merged vector is then given to the GRU\n",
    "  \n",
    "The shapes of all the vectors at each step have been specified in the comments in the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "avyJ_4VIUoHb"
   },
   "outputs": [],
   "source": [
    "def gru(units):\n",
    "  # If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)\n",
    "  # the code automatically does that.\n",
    "    if tf.test.is_gpu_available():\n",
    "        return tf.keras.layers.CuDNNGRU(units, \n",
    "                                        return_sequences=True, \n",
    "                                        return_state=True, \n",
    "                                        recurrent_initializer='glorot_uniform')\n",
    "    else:\n",
    "        return tf.keras.layers.GRU(units, \n",
    "                                   return_sequences=True, \n",
    "                                   return_state=True, \n",
    "                                   recurrent_activation='sigmoid', \n",
    "                                   recurrent_initializer='glorot_uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nZ2rI24i3jFg"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.enc_units)\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        print(\"state: {}\".format(state.shape))\n",
    "        print(\"output: {}\".format(state.shape))\n",
    "              \n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yJ_B3mhW3jFk"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.dec_units)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # used for attention\n",
    "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        \n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        # * `score = FC(tanh(FC(EO) + FC(H)))`\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\n",
    "        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n",
    "                \n",
    "        #* `attention weights = softmax(score, axis = 1)`. Softmax by default is applied on the last axis but here we want to apply it on the *1st axis*, since the shape of score is *(batch_size, max_length, 1)*. `Max_length` is the length of our input. Since we are trying to assign a weight to each input, softmax should be applied on that axis.\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        # * `context vector = sum(attention weights * EO, axis = 1)`. Same reason as above for choosing axis as 1.\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        # * `embedding output` = The input to the decoder X is passed through an embedding layer.\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        # * `merged vector = concat(embedding output, context vector)`\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output shape == (batch_size * 1, vocab)\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state, attention_weights\n",
    "        \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.dec_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P5UY8wko3jFp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13422 9856 256 1024 64\n"
     ]
    }
   ],
   "source": [
    "print(vocab_inp_size, vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_ch_71VbIRfK"
   },
   "source": [
    "## Define the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WmTHr5iV3jFr"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = 1 - np.equal(real, 0)\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "    \n",
    "#     print(\"real: {}\".format(real))\n",
    "#     print(\"pred: {}\".format(pred))\n",
    "#     print(\"mask: {}\".format(mask))\n",
    "#     print(\"loss: {}\".format(tf.reduce_mean(loss_)))\n",
    "    \n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DMVWzzsfNl4e"
   },
   "source": [
    "## Checkpoints (Object-based saving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zj8bXQTgNwrF"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './data_out/training_checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hpObfY22IddU"
   },
   "source": [
    "## Training\n",
    "\n",
    "1. Pass the *input* through the *encoder* which return *encoder output* and the *encoder hidden state*.\n",
    "2. The encoder output, encoder hidden state and the decoder input (which is the *start token*) is passed to the decoder.\n",
    "3. The decoder returns the *predictions* and the *decoder hidden state*.\n",
    "4. The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n",
    "5. Use *teacher forcing* to decide the next input to the decoder.\n",
    "6. *Teacher forcing* is the technique where the *target word* is passed as the *next input* to the decoder.\n",
    "7. The final step is to calculate the gradients and apply it to the optimizer and backpropagate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ddefjBMa3jF0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: (64, 1024)\n",
      "output: (64, 1024)\n",
      "Epoch 1 Batch 0 Loss 2.1068\n",
      "state: (64, 1024)\n",
      "output: (64, 1024)\n",
      "state: (64, 1024)\n",
      "output: (64, 1024)\n",
      "state: (64, 1024)\n",
      "output: (64, 1024)\n",
      "state: (64, 1024)\n",
      "output: (64, 1024)\n",
      "state: (64, 1024)\n",
      "output: (64, 1024)\n",
      "state: (64, 1024)\n",
      "output: (64, 1024)\n",
      "state: (64, 1024)\n",
      "output: (64, 1024)\n",
      "state: (64, 1024)\n",
      "output: (64, 1024)\n",
      "state: (64, 1024)\n",
      "output: (64, 1024)\n",
      "state: (64, 1024)\n",
      "output: (64, 1024)\n",
      "state: (64, 1024)\n",
      "output: (64, 1024)\n",
      "state: (64, 1024)\n",
      "output: (64, 1024)\n",
      "state: (64, 1024)\n",
      "output: (64, 1024)\n",
      "state: (64, 1024)\n",
      "output: (64, 1024)\n",
      "state: (64, 1024)\n",
      "output: (64, 1024)\n",
      "state: (64, 1024)\n",
      "output: (64, 1024)\n",
      "state: (64, 1024)\n",
      "output: (64, 1024)\n",
      "state: (64, 1024)\n",
      "output: (64, 1024)\n",
      "state: (64, 1024)\n",
      "output: (64, 1024)\n",
      "state: (64, 1024)\n",
      "output: (64, 1024)\n",
      "state: (64, 1024)\n",
      "output: (64, 1024)\n",
      "state: (64, 1024)\n",
      "output: (64, 1024)\n",
      "state: (64, 1024)\n",
      "output: (64, 1024)\n",
      "state: (64, 1024)\n",
      "output: (64, 1024)\n",
      "state: (64, 1024)\n",
      "output: (64, 1024)\n",
      "state: (64, 1024)\n",
      "output: (64, 1024)\n",
      "state: (64, 1024)\n",
      "output: (64, 1024)\n",
      "state: (64, 1024)\n",
      "output: (64, 1024)\n",
      "state: (64, 1024)\n",
      "output: (64, 1024)\n",
      "state: (64, 1024)\n",
      "output: (64, 1024)\n",
      "state: (64, 1024)\n",
      "output: (64, 1024)\n",
      "state: (64, 1024)\n",
      "output: (64, 1024)\n",
      "state: (64, 1024)\n",
      "output: (64, 1024)\n",
      "state: (64, 1024)\n",
      "output: (64, 1024)\n",
      "state: (64, 1024)\n",
      "output: (64, 1024)\n",
      "state: (64, 1024)\n",
      "output: (64, 1024)\n",
      "state: (64, 1024)\n",
      "output: (64, 1024)\n",
      "state: (64, 1024)\n",
      "output: (64, 1024)\n",
      "state: (64, 1024)\n",
      "output: (64, 1024)\n",
      "state: (64, 1024)\n",
      "output: (64, 1024)\n",
      "state: (64, 1024)\n",
      "output: (64, 1024)\n",
      "state: (64, 1024)\n",
      "output: (64, 1024)\n",
      "state: (64, 1024)\n",
      "output: (64, 1024)\n",
      "state: (64, 1024)\n",
      "output: (64, 1024)\n",
      "state: (64, 1024)\n",
      "output: (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "\n",
    "#GPU 설정\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(dataset):\n",
    "        loss = 0\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_output, enc_hidden = encoder(inp, hidden)\n",
    "            \n",
    "            dec_hidden = enc_hidden\n",
    "            \n",
    "            dec_input = tf.expand_dims([targ_lang.word2idx['<start>']] * BATCH_SIZE, 1)       \n",
    "            \n",
    "            # Teacher forcing - feeding the target as the next input\n",
    "            for t in range(1, targ.shape[1]):\n",
    "                # passing enc_output to the decoder\n",
    "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "                \n",
    "                loss += loss_function(targ[:, t], predictions)\n",
    "                \n",
    "                # using teacher forcing\n",
    "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "        \n",
    "        batch_loss = (loss / int(targ.shape[1]))\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        variables = encoder.variables + decoder.variables\n",
    "        \n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "          checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                        total_loss / N_BATCH))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mU3Ce8M6I3rz"
   },
   "source": [
    "## Translate\n",
    "\n",
    "* The evaluate function is similar to the training loop, except we don't use *teacher forcing* here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n",
    "* Stop predicting when the model predicts the *end token*.\n",
    "* And store the *attention weights for every time step*.\n",
    "\n",
    "Note: The encoder output is calculated only once for one input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EbQpyYs13jF_"
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    \n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    inputs = [inp_lang.word2idx[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word2idx['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        \n",
    "        # storing the attention weigths to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += targ_lang.idx2word[predicted_id] + ' '\n",
    "\n",
    "        if targ_lang.idx2word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "        \n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s5hQWlbN3jGF"
   },
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    \n",
    "    fontdict = {'fontsize': 14}\n",
    "    \n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sl9zUHzg3jGI"
   },
   "outputs": [],
   "source": [
    "def translate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    result, sentence, attention_plot = evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n",
    "        \n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    \n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n250XbnjOaqP"
   },
   "source": [
    "## Restore the latest checkpoint and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UJpT9D5_OgP6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.checkpointable.util.CheckpointLoadStatus at 0x7f93e87f71d0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WrAM0FDomq3E"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> 뭐라도 배워볼까 . <end>\n",
      "Predicted translation: 이제 그만 놓아주세요 . <end> \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate('뭐라도 배워볼까.', encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "DUQVLVqUE1YW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> trata de averiguarlo . <end>\n",
      "Predicted translation: try to figure it out . <end> \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# wrong translation\n",
    "translate('trata de averiguarlo.', encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한글 폰트가 깨질 때\n",
    "\n",
    "https://financedata.github.io/posts/matplotlib-hangul-for-ubuntu-linux.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linux-4.4.0-116-generic-x86_64-with-Ubuntu-16.04-xenial\n",
      "sys.version_info(major=3, minor=5, micro=2, releaselevel='final', serial=0)\n",
      "버전:  2.2.3\n",
      "설치위치:  /home/companyai8way/tf110/lib/python3.5/site-packages/matplotlib/__init__.py\n",
      "설정:  /home/companyai8way/.config/matplotlib\n",
      "캐시:  /home/companyai8way/.cache/matplotlib\n"
     ]
    }
   ],
   "source": [
    "#실행중인 운영체제 확인\n",
    "\n",
    "import platform\n",
    "print(platform.platform())\n",
    "\n",
    "# 파이썬 버전\n",
    "import sys\n",
    "print(sys.version_info)\n",
    "\n",
    "# matplotlib 주요 설치 정보\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "print ('버전: ', matplotlib.__version__)\n",
    "print ('설치위치: ', matplotlib.__file__)\n",
    "print ('설정: ', matplotlib.get_configdir())\n",
    "print ('캐시: ', matplotlib.get_cachedir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 우분투에 폰트는 /usr/share/fonts/ 디렉토리에 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 12\r\n",
      "drwxr-xr-x 4 root root 4096 Oct 29 02:34 \u001b[0m\u001b[01;34mtruetype\u001b[0m/\r\n",
      "drwxr-xr-x 3 root root 4096 Oct 29 00:06 \u001b[01;34mtype1\u001b[0m/\r\n",
      "drwxr-xr-x 6 root root 4096 Oct 29 00:06 \u001b[01;34mX11\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls -l /usr/share/fonts/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8\r\n",
      "drwxr-xr-x 2 root root 4096 Jun 11 15:16 \u001b[0m\u001b[01;34mdejavu\u001b[0m/\r\n",
      "drwxr-xr-x 2 root root 4096 Oct 29 02:34 \u001b[01;34mnanum\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls -l /usr/share/fonts/truetype/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 나눔 글꼴 설치\n",
    "# apt-get 명령으로 나눔글꼴(fonts-nanum)을 설치하고, fc-cache 명령으로 폰트 캐시 삭제\n",
    "\n",
    "# $ sudo apt-get install fonts-nanum*\n",
    "# $ sudo fc-cache -fv\n",
    "# 만일 다른 ttf 폰트를 가져왔다면 다음과 같이 복사하고, fc-cache 명령으로 폰트 캐시 삭제\n",
    "\n",
    "# $ sudo cp new_font.ttf /usr/share/fonts/\n",
    "# $ sudo fc-cache -fv\n",
    "# matplotlib 나눔 글꼴을 추가\n",
    "# 나눔 글꼴을 matplotlib 에 복사하고, matplotlib의 폰트 캐시를 삭제\n",
    "\n",
    "# $ sudo cp /usr/share/fonts/truetype/nanum/Nanum* /usr/local/lib/python3.4/dist-packages/matplotlib/mpl-data/fonts/ttf/\n",
    "# $ rm -rf /home/ubuntu/.cache/matplotlib/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 설치확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 133056\r\n",
      "-rw-r--r-- 1 root root  4413912 Oct  3  2014 NanumBarunGothicBold.ttf\r\n",
      "-rw-r--r-- 1 root root  4917332 Oct  3  2014 NanumBarunGothicLight.ttf\r\n",
      "-rw-r--r-- 1 root root  4184028 Oct  3  2014 NanumBarunGothic.ttf\r\n",
      "-rw-r--r-- 1 root root  4790084 Oct  3  2014 NanumBarunGothicUltraLight.ttf\r\n",
      "-rw-r--r-- 1 root root 10028540 Oct  3  2014 NanumBarunpenB.ttf\r\n",
      "-rw-r--r-- 1 root root 11033840 Oct  3  2014 NanumBarunpenR.ttf\r\n",
      "-rw-r--r-- 1 root root  3745376 Oct  3  2014 NanumBrush.ttf\r\n",
      "-rw-r--r-- 1 root root  4288380 Oct  3  2014 NanumGothicBold.ttf\r\n",
      "-rw-r--r-- 1 root root  2246240 May  6  2010 NanumGothic_Coding_Bold.ttf\r\n",
      "-rw-r--r-- 1 root root  2315924 May  6  2010 NanumGothic_Coding.ttf\r\n",
      "-rw-r--r-- 1 root root 11021172 Oct 29  2011 NanumGothicEcoBold.ttf\r\n",
      "-rw-r--r-- 1 root root 11571064 Oct 29  2011 NanumGothicEcoExtraBold.ttf\r\n",
      "-rw-r--r-- 1 root root  7783984 Oct 29  2011 NanumGothicEco.ttf\r\n",
      "-rw-r--r-- 1 root root  4171172 Oct  3  2014 NanumGothicExtraBold.ttf\r\n",
      "-rw-r--r-- 1 root root  1523188 Oct  3  2014 NanumGothicLight.ttf\r\n",
      "-rw-r--r-- 1 root root  4343844 Oct  3  2014 NanumGothic.ttf\r\n",
      "-rw-r--r-- 1 root root  4183592 Oct  3  2014 NanumMyeongjoBold.ttf\r\n",
      "-rw-r--r-- 1 root root  9809868 Oct 29  2011 NanumMyeongjoEcoBold.ttf\r\n",
      "-rw-r--r-- 1 root root 10635180 Oct 29  2011 NanumMyeongjoEcoExtraBold.ttf\r\n",
      "-rw-r--r-- 1 root root  7024632 Oct 29  2011 NanumMyeongjoEco.ttf\r\n",
      "-rw-r--r-- 1 root root  4788696 Oct  3  2014 NanumMyeongjoExtraBold.ttf\r\n",
      "-rw-r--r-- 1 root root  3839464 Oct  3  2014 NanumMyeongjo.ttf\r\n",
      "-rw-r--r-- 1 root root  3548656 Oct  3  2014 NanumPen.ttf\r\n"
     ]
    }
   ],
   "source": [
    "ls -l /usr/share/fonts/truetype/nanum/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NanumGothicCoding',\n",
       "  '/home/companyai8way/tf110/lib/python3.5/site-packages/matplotlib/mpl-data/fonts/ttf/NanumGothic_Coding_Bold.ttf'),\n",
       " ('NanumBarunpen',\n",
       "  '/home/companyai8way/tf110/lib/python3.5/site-packages/matplotlib/mpl-data/fonts/ttf/NanumBarunpenR.ttf'),\n",
       " ('NanumMyeongjo',\n",
       "  '/home/companyai8way/tf110/lib/python3.5/site-packages/matplotlib/mpl-data/fonts/ttf/NanumMyeongjo.ttf'),\n",
       " ('NanumMyeongjo Eco',\n",
       "  '/home/companyai8way/tf110/lib/python3.5/site-packages/matplotlib/mpl-data/fonts/ttf/NanumMyeongjoEco.ttf'),\n",
       " ('Nanum Brush Script',\n",
       "  '/home/companyai8way/tf110/lib/python3.5/site-packages/matplotlib/mpl-data/fonts/ttf/NanumBrush.ttf'),\n",
       " ('NanumMyeongjo Eco',\n",
       "  '/home/companyai8way/tf110/lib/python3.5/site-packages/matplotlib/mpl-data/fonts/ttf/NanumMyeongjoEcoBold.ttf'),\n",
       " ('NanumGothic',\n",
       "  '/home/companyai8way/tf110/lib/python3.5/site-packages/matplotlib/mpl-data/fonts/ttf/NanumGothicExtraBold.ttf'),\n",
       " ('NanumBarunpen',\n",
       "  '/home/companyai8way/tf110/lib/python3.5/site-packages/matplotlib/mpl-data/fonts/ttf/NanumBarunpenB.ttf'),\n",
       " ('NanumBarunGothic',\n",
       "  '/home/companyai8way/tf110/lib/python3.5/site-packages/matplotlib/mpl-data/fonts/ttf/NanumBarunGothicLight.ttf'),\n",
       " ('NanumBarunGothic',\n",
       "  '/home/companyai8way/tf110/lib/python3.5/site-packages/matplotlib/mpl-data/fonts/ttf/NanumBarunGothicBold.ttf'),\n",
       " ('NanumGothic Eco',\n",
       "  '/home/companyai8way/tf110/lib/python3.5/site-packages/matplotlib/mpl-data/fonts/ttf/NanumGothicEcoBold.ttf'),\n",
       " ('NanumGothic',\n",
       "  '/home/companyai8way/tf110/lib/python3.5/site-packages/matplotlib/mpl-data/fonts/ttf/NanumGothicLight.ttf'),\n",
       " ('NanumGothic Eco',\n",
       "  '/home/companyai8way/tf110/lib/python3.5/site-packages/matplotlib/mpl-data/fonts/ttf/NanumGothicEco.ttf'),\n",
       " ('NanumMyeongjo Eco',\n",
       "  '/home/companyai8way/tf110/lib/python3.5/site-packages/matplotlib/mpl-data/fonts/ttf/NanumMyeongjoEcoExtraBold.ttf'),\n",
       " ('NanumGothic',\n",
       "  '/home/companyai8way/tf110/lib/python3.5/site-packages/matplotlib/mpl-data/fonts/ttf/NanumGothic.ttf'),\n",
       " ('NanumGothic',\n",
       "  '/home/companyai8way/tf110/lib/python3.5/site-packages/matplotlib/mpl-data/fonts/ttf/NanumGothicBold.ttf'),\n",
       " ('NanumBarunGothic',\n",
       "  '/home/companyai8way/tf110/lib/python3.5/site-packages/matplotlib/mpl-data/fonts/ttf/NanumBarunGothicUltraLight.ttf'),\n",
       " ('NanumBarunGothic',\n",
       "  '/home/companyai8way/tf110/lib/python3.5/site-packages/matplotlib/mpl-data/fonts/ttf/NanumBarunGothic.ttf'),\n",
       " ('Nanum Pen Script',\n",
       "  '/home/companyai8way/tf110/lib/python3.5/site-packages/matplotlib/mpl-data/fonts/ttf/NanumPen.ttf'),\n",
       " ('NanumMyeongjo',\n",
       "  '/home/companyai8way/tf110/lib/python3.5/site-packages/matplotlib/mpl-data/fonts/ttf/NanumMyeongjoExtraBold.ttf'),\n",
       " ('NanumGothic Eco',\n",
       "  '/home/companyai8way/tf110/lib/python3.5/site-packages/matplotlib/mpl-data/fonts/ttf/NanumGothicEcoExtraBold.ttf'),\n",
       " ('NanumGothicCoding',\n",
       "  '/home/companyai8way/tf110/lib/python3.5/site-packages/matplotlib/mpl-data/fonts/ttf/NanumGothic_Coding.ttf'),\n",
       " ('NanumMyeongjo',\n",
       "  '/home/companyai8way/tf110/lib/python3.5/site-packages/matplotlib/mpl-data/fonts/ttf/NanumMyeongjoBold.ttf')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matplotlib.font_manager.fontManager.ttflist\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.font_manager\n",
    "\n",
    "[f.fname for f in matplotlib.font_manager.fontManager.ttflist]\n",
    "[(f.name, f.fname) for f in matplotlib.font_manager.fontManager.ttflist if 'Nanum' in f.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NanumGothic\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import font_manager, rc\n",
    "\n",
    "font_fname = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'\n",
    "font_name = font_manager.FontProperties(fname=font_fname).get_name()\n",
    "\n",
    "rc('font', family=font_name)\n",
    "print(font_name)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "nmt_with_attention.ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1C4fpM7_7IL8ZzF7Gc5abywqQjeQNS2-U",
     "timestamp": 1527858391290
    },
    {
     "file_id": "1pExo6aUuw0S6MISFWoinfJv0Ftm9V4qv",
     "timestamp": 1527776041613
    }
   ],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
